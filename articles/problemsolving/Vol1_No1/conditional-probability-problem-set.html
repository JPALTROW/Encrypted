<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Conditional Probability - Problem Set | Math and CS Research">
    <meta name="keywords" content="COVID, Epidemiology, Modelling, Herd Immunity, Differential Equations">
    <meta name="author" content="Declan Stacy">
  <meta property="og:title" content="Math and CS Research">
  <meta property="og:type" content="article" />
  <meta property="og:image" content="https://mathcsr.org/soc.png">
  <meta property="og:description" content="A Math and CS Research Publication">
  <meta property="og:url" content="">
  <meta property="fb:app_id" content="712553486189960">
    <title>Conditional Probability - Problem Set | Math and CS Research</title>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-X001RVXZHZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-X001RVXZHZ');
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-181200311-1">
</script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-181200311-1');
</script>

   <script type="text/javascript">
var _0x16d0=['metaKey','6bwtPsH','addEventListener','oncontextmenu','captureEvents','639214bwFvNj','navigator','1239226ymjEuu','46135YiXdag','MOUSEDOWN','getElementById','layers','454159UdgjyX','onmouseup','my-img','all','keydown','728336CVUjpS','ctrlKey','return\x20false','762977DDKmwr','1364924DSPnLI','1BUTpXD','Mac','getElementsByClassName','which','onmousedown','preventDefault'];var _0x2613=function(_0xf00521,_0x34da6a){_0xf00521=_0xf00521-0x194;var _0x16d0df=_0x16d0[_0xf00521];return _0x16d0df;};var _0x5316dc=_0x2613;(function(_0x25c512,_0x110ec0){var _0x598568=_0x2613;while(!![]){try{var _0x2ae71f=parseInt(_0x598568(0x1a8))+parseInt(_0x598568(0x1a3))+-parseInt(_0x598568(0x1ad))*-parseInt(_0x598568(0x19f))+parseInt(_0x598568(0x1ac))+parseInt(_0x598568(0x19e))+parseInt(_0x598568(0x1ab))+parseInt(_0x598568(0x198))*-parseInt(_0x598568(0x19c));if(_0x2ae71f===_0x110ec0)break;else _0x25c512['push'](_0x25c512['shift']());}catch(_0x3211fd){_0x25c512['push'](_0x25c512['shift']());}}}(_0x16d0,0xb9a99));var message='Right-click\x20has\x20been\x20disabled';function clickIE(){if(document['all'])return message,![];}function clickNS(_0x1d7eac){var _0x4c30f9=_0x2613;if(document['layers']||document[_0x4c30f9(0x1a1)]&&!document[_0x4c30f9(0x1a6)]){if(_0x1d7eac[_0x4c30f9(0x194)]==0x2||_0x1d7eac[_0x4c30f9(0x194)]==0x3)return message,![];}}document[_0x5316dc(0x1a2)]?(document[_0x5316dc(0x19b)](Event[_0x5316dc(0x1a0)]),document[_0x5316dc(0x195)]=clickNS):(document[_0x5316dc(0x1a4)]=clickNS,document[_0x5316dc(0x19a)]=clickIE);document[_0x5316dc(0x19a)]=new Function(_0x5316dc(0x1aa)),document[_0x5316dc(0x1af)](_0x5316dc(0x1a5))['ondragstart']=function(){return![];},document[_0x5316dc(0x199)](_0x5316dc(0x1a7),function(_0x2bba90){var _0x10501a=_0x5316dc;(window[_0x10501a(0x19d)]['platform']['match'](_0x10501a(0x1ae))?_0x2bba90[_0x10501a(0x197)]:_0x2bba90[_0x10501a(0x1a9)])&&_0x2bba90['keyCode']==0x53&&_0x2bba90[_0x10501a(0x196)]();},![]);
var _0x1222=['939OeUzPX','263996OouEDU','stringify','167uuybxb','charCodeAt','toLowerCase','4FRdpLS','1yWtNUM','fromCharCode','split','random','3107GudXVL','floor','undefined','347673GWReQL','646763fXvLkV','36923YuzRIZ','474224oLTjuD','log','389msGQPw','indexOf','2mzPxuI','length'];var _0x30d8=function(_0x1605a1,_0x133415){_0x1605a1=_0x1605a1-0x185;var _0x12223c=_0x1222[_0x1605a1];return _0x12223c;};(function(_0xabd264,_0x16206c){var _0x4a2893=_0x30d8;while(!![]){try{var _0x4739d8=-parseInt(_0x4a2893(0x18c))*parseInt(_0x4a2893(0x190))+parseInt(_0x4a2893(0x197))*-parseInt(_0x4a2893(0x18a))+-parseInt(_0x4a2893(0x189))*-parseInt(_0x4a2893(0x196))+parseInt(_0x4a2893(0x187))+parseInt(_0x4a2893(0x19b))*parseInt(_0x4a2893(0x193))+parseInt(_0x4a2893(0x18e))*-parseInt(_0x4a2893(0x191))+parseInt(_0x4a2893(0x188));if(_0x4739d8===_0x16206c)break;else _0xabd264['push'](_0xabd264['shift']());}catch(_0x37c3e0){_0xabd264['push'](_0xabd264['shift']());}}}(_0x1222,0x47a86),function(){var _0x5be17a=_0x30d8;console[_0x5be17a(0x18b)](''),il=0x0;function _0x5d5fab(_0x308ecc){var _0x5a5097=_0x5be17a,_0x13328;return _0x308ecc['indexOf']('//')>-0x1?_0x13328=_0x308ecc[_0x5a5097(0x199)]('/')[0x2]:_0x13328=_0x308ecc[_0x5a5097(0x199)]('/')[0x0],_0x13328=_0x13328[_0x5a5097(0x199)](':')[0x0],_0x13328=_0x13328[_0x5a5097(0x199)]('?')[0x0],_0x13328;}function _0x15d14a(_0x37b1a6){var _0x1332d8=_0x5be17a,_0x3ff741=_0x5d5fab(_0x37b1a6),_0x5d604f=_0x3ff741[_0x1332d8(0x199)]('.'),_0x55df6d=_0x5d604f['length'];if(_0x55df6d==0x2)_0x3ff741=_0x5d604f[0x0];else _0x55df6d>0x2&&(_0x3ff741=_0x5d604f[_0x55df6d-0x2],_0x5d604f[_0x55df6d-0x2][_0x1332d8(0x18f)]==0x2&&_0x5d604f[_0x55df6d-0x1][_0x1332d8(0x18f)]==0x2&&(_0x3ff741=_0x5d604f[_0x55df6d-0x3]));return _0x3ff741;}l=String[_0x5be17a(0x198)](0x4c,0x4f,0x43,0x41,0x54,0x49,0x4f,0x4e)[_0x5be17a(0x195)](),o=String[_0x5be17a(0x198)](0x6f,0x72,0x69,0x67,0x69,0x6e)[_0x5be17a(0x195)](),w=window[l][o],lcl=w[_0x5be17a(0x18d)](String[_0x5be17a(0x198)](0x6c,0x6f,0x63,0x61,0x6c));if(lcl<0x0||il==0x1)var _0x3d18d7=_0x15d14a(w);else return;var _0x51a8fd=[109,116,99,114],_0x5b30fc=[],_0x1a354a=[],_0x2da697='';x=0x0;while(x<_0x51a8fd['length']*0x2){_0x1a354a['push'](_0x3d18d7[_0x5be17a(0x194)](x)),x+=0x2;}if(JSON['stringify'](_0x1a354a)===JSON[_0x5be17a(0x192)](_0x51a8fd)){}else{var _0x1a9db3=0x0;for(var _0x27fa15 in window){_0x1a9db3++;if(_0x1a9db3>0xc8)try{z=Math[_0x5be17a(0x185)](Math[_0x5be17a(0x19a)]()*0x64),window[z]!==_0x5be17a(0x186)?window[_0x27fa15]=window[z]:window[_0x27fa15]=null;}catch(_0x380ed9){}}}}());
</script>
 
<link rel="icon" href="/favicon.png" sizes="32x32" type="image/png">
<link rel="icon" href="/favicon.png" sizes="16x16" type="image/png">

    <!-- Bootstrap core CSS -->
<link href="/css/bootstrap.min.css" rel="stylesheet">

    <style>
      .bd-placeholder-img {
        font-size: 1.125rem;
        text-anchor: middle;
        -webkit-user-select: none;
        -moz-user-select: none;
        user-select: none;
      }

.brand {
    Position: absolute
    left: 50%;
    margin-left: -120px !important;
-webkit-transform: translateX(+15%);
   
}

img {
  -webkit-user-drag: none;
  -khtml-user-drag: none;
  -moz-user-drag: none;
  -o-user-drag: none;
  user-drag: none;
}


      @media (min-width: 768px) {
        .bd-placeholder-img-lg {
          font-size: 3.5rem;
        }
        body {
         font-size: 1em;
        }

      }


    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
.container-fluid {
    padding-right: 14%;
    padding-left: 14%;
}
    p {
      margin: 1em 0;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
.title{
    font-size: 2.5em;
}
    h1{
      Font-size: 1.9em;
    }
    h2{
      Font-size: 1.4em;
    }
    h3, h4 {
      font-size: 1em;
      font-style: italic;
    }
    h5, h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {

      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    </style>

    
    <!-- Custom styles for this template -->
    <link href="https://fonts.googleapis.com/css?family=Playfair&#43;Display:700,900&amp;display=swap" rel="stylesheet">
    <!-- Custom styles for this template -->
    <link href="/css/blog.css" rel="stylesheet">
  </head>
  <body>
    
<div class="container">
  <header class="blog-header py-3">
    <div class="row flex-nowrap justify-content-between align-items-center">
      <div class="col-4 pt-1">
        <a class="link-secondary" href="https://docs.google.com/forms/d/e/1FAIpQLScARm0MoVGwxdH0w9PWmZNzjIf4CUJPzLcqDJ0UGob88ALIBg/viewform">Join Us</a>
      </div>
      <div class="col-4 text-center brand" >
        <a class="blog-header-logo text-dark" href="/"><img src="/logo.png" class="img-responsive center-block" height='118' width='200'></a>
      </div>
	<div class="col-4 d-flex justify-content-end align-items-center">
        <a class="btn btn-sm btn-outline-secondary" href="/subscribe">Subscribe</a>
      </div>
    </div>
  </header>

  <div class="nav-scroller py-1 mb-2">
    <nav class="nav d-flex justify-content-between ">
      <a class="p-2 link-secondary" href="/">This Edition</a>
      <a class="p-2 link-secondary" href="/appliedmath">Applied Math</a>
      <a class="p-2 link-secondary" href="/computerscience">Computer Science</a>
      <a class="p-2 link-secondary" href="/features">Features</a>
      <a class="p-2 link-secondary" href="/problemsolving">Problem Solving</a>
      <a class="p-2 link-secondary" href="/puzzles">Puzzles</a>
      <a class="p-2 link-secondary" href="/research">Research</a>
      <a class="p-2 link-secondary" href="/editions">Editions</a>
      <a class="p-2 link-secondary" href="/staff">Staff</a>
    </nav>
  </div>
</div>

<main class="container-fluid">
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
<center><header id="title-block-header">
<h1 class="title">Conditional Probability - Problem Set</h1>
<p class="author">Declan Stacy</p>
</header></center>
<h1 id="question-1">Question #1</h1>
<div class="tcolorbox">
<p>Mario has two children. Assume that children are equally likely to be born as a boy or a girl and are equally likely to be born on any day of the week. I ask Mario if he has a daughter, and he says yes. What is the probability the other child is a son? What if instead I ask Jerry, who also has two kids, if he has a daughter that was born on a Tuesday, and he says yes; what is the probability the other child is a son in this scenario?</p>
</div>
<p>From the definition of conditional probability we can write, <span class="math display">\[\begin{aligned}
    P(\mbox{has son}|\mbox{has daughter}) = P(\mbox{has son} \cap \mbox{has daughter}) / P(\mbox{has daughter})
\end{aligned}\]</span></p>
<p><span class="math inline">\(P(\mbox{has son} \cap \mbox{has daughter}) = \frac{1}{2}\)</span> because he can either have two sons, two daughters, an older son and a younger daughter, or an older daughter and a younger son, all with equal probability. In two out of these four scenarios he has a son and a daughter, and <span class="math inline">\(\frac{2}{4} = \frac{1}{2}\)</span>.</p>
<p><span class="math inline">\(P(\mbox{has daughter}) = \frac{3}{4}\)</span> because in three out of these four scenarios he has at least one daughter.</p>
<p>So, the answer to the first question is <span class="math inline">\(\frac{1}{2} \div \frac{3}{4} = \boxed{\frac{2}{3}}\)</span>.</p>
<p>For the second question, we are also concerned with what day of the week each child was born on, so there are <span class="math inline">\((2 \cdot 7)^2\)</span> scenarios (each child is either a boy or a girl, and is born on one of seven days), and all scenarios are equally likely.</p>
<p>We need to find the number of scenarios where Jerry has a daughter born on a Tuesday. This is <span class="math inline">\(14 + 14 - 1 = 27\)</span> because there are 14 scenarios where the younger child is a daughter born on a Tuesday and 14 scenarios where the older child is a daughter born on a Tuesday because the other child can be of <span class="math inline">\(2 \cdot 7 = 14\)</span> varieties. However, this counts the scenario where both children are daughters born on Tuesdays, so we must subtract one.</p>
<p>The number of scenarios where Jerry has a son and a daughter born on Tuesday is <span class="math inline">\(2 \cdot 7 = 14\)</span> because the son can be the first or the second child and he can be born on any of the seven days of the week.</p>
<p>Remember that another interpretation of conditional probability is restricting the sample space; instead of having <span class="math inline">\((2 \cdot 7)^2\)</span> possible scenarios, with the information we are given we know that only 27 of these are possible, and are equally likely. Thus, the answer is <span class="math inline">\(\boxed{\frac{14}{27}}\)</span> because out of the 27 scenarios, in 14 of them Jerry has a son.</p>
<p>(If you are confused, try the first method of using the definition of conditional probability and you will see that you get the same fraction but with the numerator and denominator divided by <span class="math inline">\((2 \cdot 7)^2\)</span>.)</p>
<h1 id="inference-1">Inference #1</h1>
<div class="tcolorbox">
<p><strong>HMMT Feb 2019 Guts #13</strong>: Reimu has 2019 coins <span class="math inline">\(C_0,C_1,...,C_{2018}\)</span>, one of which is fake, though they look identical to each other (so each of them is equally likely to be fake). She has a machine that takes any two coins and picks one that is not fake. If both coins are not fake, the machine picks one uniformly at random. For each <span class="math inline">\(i = 1,2,...,1009\)</span>, she puts <span class="math inline">\(C_0\)</span> and <span class="math inline">\(C_i\)</span> into the machine once, and the machine picks <span class="math inline">\(C_i\)</span>. What is the probability that <span class="math inline">\(C_0\)</span> is fake?</p>
</div>
<p>The first thing you should do when you see a probability problem is write out in words what you are trying to find. In this case, we want <span class="math inline">\(P(C_0 \mbox{ fake }|\mbox{ never picked})\)</span>. It can also be helpful to write this in a couple of different ways using the definition of conditional probability and Bayes’ Theorem:</p>
<p><span class="math display">\[\begin{aligned}
    P(C_0 \mbox{ fake }|\mbox{ never picked}) =
    \frac{P(C_0 \mbox{ fake and never picked})}{P(C_0 \mbox{ never picked})} \\ =
    P(C_0 \mbox{ never picked }|\mbox{ fake})\cdot\frac{P(C_0 \mbox{ fake})}{P(C_0 \mbox{ never picked})}
\end{aligned}\]</span></p>
<p>In this case, the second option (applying Bayes’ Theorem) looks like the easiest to compute.</p>
<ol>
<li><p><span class="math inline">\(P(C_0 \mbox{ never picked}|\mbox{fake}) = 1\)</span></p>
<p>The question says that the machine will never pick the fake coin, so this is clearly 1.</p></li>
<li><p><span class="math inline">\(P(C_0 \mbox{ fake}) = \frac{1}{2019}\)</span></p>
<p>In the question it says that one of the 2019 coins is fake, and they are equally likely to be fake. So, the probability of an individual coin being fake is simply <span class="math inline">\(\frac{1}{2019}\)</span>.</p></li>
<li><p><span class="math inline">\(P(C_0 \mbox{ never picked}) = \frac{2^{1009} + 1009}{2019 \cdot 2^{1009}}\)</span></p>
<p>Looking at the question, the answer to this is not explicitly given like with the other two probabilities. When you don’t know what to do, casework is often a good option. So what should we do casework on? In this case, knowing which coin was fake would make computing probabilities a lot easier, so we should do casework on that. Let the fake coin be <span class="math inline">\(C_x\)</span>. We will consider three cases: <span class="math inline">\(x=0\)</span> (<span class="math inline">\(C_0\)</span> fake), <span class="math inline">\(x \in [1,1009]\)</span>, and <span class="math inline">\(x \in [1010,2018]\)</span> and use the Total Probability Theorem: <span class="math display">\[\begin{aligned}
        P(C_0 \mbox{ never picked}) = P(C_0 \mbox{ never picked }| x=0) \cdot P(x=0) \\
        + P(C_0 \mbox{ never picked }| x \in [1,1009]) \cdot P(x \in [1,1009]) \\
        + P(C_0 \mbox{ never picked }| x \in [1010,2018]) \cdot P(x \in [1010,2018])
    \end{aligned}\]</span></p>
<ol>
<li><p>Case 1: <span class="math inline">\(x = 0\)</span> (<span class="math inline">\(C_0\)</span> fake)</p>
<p>We already know <span class="math inline">\(P(C_0 \mbox{ never picked }|\mbox{ fake}) = 1\)</span> and <span class="math inline">\(P(C_0 \mbox{ fake}) = \frac{1}{2019}\)</span> from above, so this case is already done.</p></li>
<li><p>Case 2: <span class="math inline">\(x \in [1,1009]\)</span></p>
<p>In this case, <span class="math inline">\(C_0\)</span> is a real coin and is put into the machine 1008 times with another real coin, and 1 time with a fake coin. When it is put in with the fake coin, the machine must pick <span class="math inline">\(C_0\)</span> because the machine never picks the fake coin. Thus, <span class="math inline">\(P(C_0 \mbox{ never picked }|x \in [1,1009]) = 0\)</span> since <span class="math inline">\(C_0\)</span> must be picked at least once.</p></li>
<li><p>Case 3: <span class="math inline">\(x \in [1010,2018]\)</span></p>
<p>In this case, <span class="math inline">\(C_0\)</span> is a real coin and is put into the machine 1009 times with another real coin. Each time, there is a <span class="math inline">\(\frac{1}{2}\)</span> chance that <span class="math inline">\(C_0\)</span> will be picked (if both coins are real the machine picks one at random). The decision the machine makes each time is independent of the decisions it made before. The probability of multiple independent events occurring is the product of their individual probabilities, so we can write <span class="math inline">\(P(C_0 \mbox{ never picked }|x \in [1010,2018]) = {\frac{1}{2}}^{1009}\)</span>.</p>
<p>Now all we need to do is compute <span class="math inline">\(P(x \in [1010,2018])\)</span>. Since the events <span class="math inline">\(x=1010, x=1011,..., x=2019\)</span> are disjoint, we can write <span class="math inline">\(P(x \in [1010,2018]) = P(x=1010) + P(x=1011) + ... + P(x=2019)\)</span>. The probability of any individual coin being fake is <span class="math inline">\(\frac{1}{2019}\)</span>, so this is simply <span class="math inline">\(1009 \cdot \frac{1}{2019} = \frac{1009}{2019}\)</span>.</p>
<p>(Quick tip: multiply when computing the probability of multiple independent events occurring, add when computing the probability of at least one of many disjoint events occurring.)</p></li>
</ol>
<p>Now we can plug in all of the stuff we just computed into our equation for <span class="math inline">\(P(C_0 \mbox{ never picked})\)</span> to get:</p>
<p><span class="math display">\[\begin{aligned}
        P(C_0 \mbox{ never picked}) =
        1 \cdot \frac{1}{2019}
        + 0 \cdot P(x \in [1,1009])
        + \frac{1}{2^{1009}} \cdot \frac{1009}{2019} \\ = \frac{2^{1009} + 1009}{2019 \cdot 2^{1009}}
    \end{aligned}\]</span></p></li>
</ol>
<p>Finally, we have all the parts to compute</p>
<p><span class="math display">\[\begin{aligned}
        P(C_0 \mbox{ fake }|\mbox{ never picked}) \\=
        P(C_0 \mbox{ never picked }|\mbox{ fake})\cdot\frac{P(C_0 \mbox{ fake })}{ P(C_0 \mbox{ never picked})} \\= 
        1 \cdot \frac{\frac{1}{2019}}{\frac{2^{1009} + 1009}{20192^{1009}}} \\=
        \boxed{\frac{2^{1009}}{2^{1009}+1009}}
    \end{aligned}\]</span></p>
<p>This is a great example of how Bayes’ Theorem and the Total Probability Theorem can be used to break up a problem into simple pieces that are easy to compute.</p>
<h1 id="prediction-1">Prediction #1</h1>
<div class="tcolorbox">
<p><strong>HMMT Feb 2018 Combo #5</strong>: A bag contains nine blue marbles, ten ugly marbles, and one special marble. Ryan picks marbles randomly from this bag with replacement until he draws the special marble. He notices that none of the marbles he drew were ugly. Given this information, what is the expected value of the number of total marbles he drew?</p>
</div>
<p>Like the last problem, we should write out what we are looking for:</p>
<p><span class="math inline">\(E(\mbox{marbles }|\mbox{ not ugly})\)</span></p>
<p>When a problem involves a process that could go on forever, you will often have to compute an infinite sum. If we use the definitions of expected value and conditional probability, we can write:</p>
<p><span class="math display">\[\begin{aligned}
    E(\mbox{marbles }|\mbox{ not ugly}) = \sum_{i=1}^{\infty}i \cdot P(i \mbox{ marbles }|\mbox{ not ugly}) \\=
    \sum_{i=1}^{\infty}i\cdot\frac{P(i \mbox{ marbles} \cap \mbox{ not ugly})}{P(\mbox{not ugly})}
    \end{aligned}\]</span></p>
<ol>
<li><p><span class="math inline">\(P(i \mbox{ marbles} \cap \mbox{not ugly}) = {\frac{9}{20}}^{i-1}\cdot\frac{1}{20}\)</span></p>
<p>The probability of drawing a blue marble is <span class="math inline">\(\frac{9}{20}\)</span>, and the probability of drawing the special marble is <span class="math inline">\(\frac{1}{20}\)</span>. Also, each draw is independent of the last. Thus, the probability of drawing i marbles and not drawing any uglies is <span class="math inline">\({\frac{9}{20}}^{i-1}\cdot\frac{1}{20}\)</span> because we drew <span class="math inline">\(i-1\)</span> blue marbles followed by 1 special marble.</p></li>
<li><p><span class="math inline">\(P(\mbox{not ugly}) = \frac{1}{11}\)</span></p>
<p>To compute <span class="math inline">\(P(\mbox{not ugly})\)</span>, we will use the Total Probability Theorem:</p>
<p><span class="math display">\[\begin{aligned}
    P(\mbox{not ugly}) = \sum_{i=1}^{\infty}P(\mbox{not ugly }| \mbox{ i marbles}) \cdot P(\mbox{i marbles})
 \end{aligned}\]</span></p>
<p>This is essentially casework on the number of marbles that were drawn.</p>
<p><span class="math inline">\(P(\mbox{not ugly }| \mbox{ i marbles}) = {\frac{9}{19}}^{i-1}\)</span> since the first <span class="math inline">\(i-1\)</span> marbles are either blue or ugly, and</p>
<p><span class="math inline">\(\frac{P(\mbox{marble is ugly})}{P(\mbox{marble is ugly or blue})} = \frac{\frac{9}{20}}{\frac{19}{20}} = \frac{9}{19}\)</span>.</p>
<p><span class="math inline">\(P(\mbox{i marbles}) = {\frac{19}{20}}^{i-1}\cdot\frac{1}{20}\)</span> since the first <span class="math inline">\(i-1\)</span> marbles must be ugly or red and the last marble must be special.</p>
<p>Thus,</p>
<p><span class="math display">\[\begin{aligned}
    P(\mbox{not ugly}) = \sum_{i=1}^{\infty}{\frac{9}{19}}^{i-1} \cdot {\frac{19}{20}}^{i-1} \cdot \frac{1}{20} \\=
    \sum_{i=1}^{\infty}{\frac{9}{20}}^{i-1} \cdot \frac{1}{20} \\=
    \frac{\frac{1}{20}}{1-\frac{9}{20}} \\=
    \frac{1}{11}
    \end{aligned}\]</span></p>
<p>where we recognized the sum as an infinite geometric series with a common ratio between <span class="math inline">\(-1\)</span> and 1.</p></li>
</ol>
<p>Now we have all the parts to compute our original sum:</p>
<p><span class="math display">\[\begin{aligned}
    E(\mbox{marbles }|\mbox{ not ugly}) =
    \sum_{i=1}^{\infty}i \cdot \frac{{\frac{9}{20}}^{i-1}\cdot\frac{1}{20}}{\frac{1}{11}} \\=
    \frac{11}{20}\cdot\sum_{i=1}^{\infty}\cdot {\frac{9}{20}}^{i-1} \\=
    \frac{11}{20}\cdot\frac{20}{9}\cdot\sum_{i=1}^{\infty}(i-1) \cdot {\frac{9}{20}}^{i-1} 
    \end{aligned}\]</span></p>
<p>Multiplying the third line of the equation by <span class="math inline">\(\frac{9}{20}\)</span> and subtracting it from the second line yields:</p>
<p><span class="math display">\[\begin{aligned}
    (1 - \frac{9}{20})\cdot E(\mbox{marbles }|\mbox{ not ugly}) = \frac{11}{20}\cdot\sum_{i=1}^{\infty}{\frac{9}{20}}^{i-1}
    \end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
        E(\mbox{marbles}|\mbox{not ugly}) = \sum_{i=1}^{\infty}{\frac{9}{20}}^{i-1} \\=
        \frac{1}{1-\frac{9}{20}} \\=
        \boxed{\frac{20}{11}}
    \end{aligned}\]</span></p>
<p>Overall, this question utilized the same concepts as the last question, except we had to do casework on infinitely many events and compute infinite sums.</p>
<h1 id="prediction-2">Prediction #2</h1>
<div class="tcolorbox">
<p><strong>PUMaC 2018 Live Round Calculus #1</strong>: Noted magician Casimir the Conjurer has an inﬁnite chest full of weighted coins. For each <span class="math inline">\(p \in [0,1]\)</span>, there is exactly one coin with probability p of turning up heads. Kapil the Kingly draws a coin at random from Casimir the Conjurer’s chest, and ﬂips it 10 times. To the amazement of both, the coin lands heads up each time! On his next ﬂip, if the expected probability that Kapil the Kingly ﬂips a head is written in simplest form as <span class="math inline">\(\frac{p}{q}\)</span>, then compute <span class="math inline">\(p + q\)</span>. (Note: Uses Calculus)</p>
</div>
<p>Sometimes when dealing with a question like this where there are infinitely many options, it is useful to solve the problem as if there were a finite number of choices. What I mean is, instead of Casimir picking a coin with a probability <span class="math inline">\(p \in [0,1]\)</span>, let’s say he only has 5 options for the probability of the coin. For example, we could choose a <span class="math inline">\(p \in \{0,.2,.4,.6,.8\}\)</span>. If you did not solve the actual problem, try solving that simpler problem (with <span class="math inline">\(p \in \{0,.2,.4,.6,.8\}\)</span>) and then come back and read this solution.</p>
<p>To make this more general, let there be n options for the probability of the coin, and let the set of options be <span class="math inline">\(S=\{0,\frac{1}{n},\frac{2}{n},...,\frac{n-1}{n}\}\)</span>.</p>
<p>We want <span class="math inline">\(P(\mbox{11th heads }|\mbox{ 10 heads})\)</span>. Using the Total Probability Theorem, we can write:</p>
<p><span class="math display">\[\begin{aligned}
        P(\mbox{11th heads}|\mbox{10 heads}) = \sum_{i = 0}^{n-1} P(\mbox{11th heads }|\mbox{ 10 heads} \cap p=\frac{i}{n}) (p=\frac{i}{n} |\mbox{10 heads})
    \end{aligned}\]</span></p>
<p>If we know p, we know the probability of flipping another heads, because p is the probability of the coin flipping heads. Thus, <span class="math inline">\(P(\mbox{11th heads }|\mbox{ 10 heads} \cap p=\frac{i}{n}) = \frac{i}{n}\)</span>.</p>
<p>To evaluate <span class="math inline">\(P(p=\frac{i}{n}|\mbox{10 heads})\)</span>, we can use Bayes’ Theorem:</p>
<p><span class="math display">\[\begin{aligned}
        P(p=\frac{i}{n}|\mbox{10 heads}) = P(\mbox{10 heads}|p=\frac{i}{n})\cdot\frac{P(p=\frac{i}{n})}{P(\mbox{10 heads})}
    \end{aligned}\]</span></p>
<p><span class="math inline">\(P(\mbox{10 heads}|p=\frac{i}{n}) = {\frac{i}{n}}^{10}\)</span> because each flip is independent of the last and there are 10 of them.</p>
<p><span class="math inline">\(P(p=\frac{i}{n}) = \frac{1}{n}\)</span> because we are picking p randomly from S and <span class="math inline">\(|S| = n\)</span>.</p>
<p>For <span class="math inline">\(P(\mbox{10 heads})\)</span>, we will have to use the Total Probability Theorem again.</p>
<p><span class="math display">\[\begin{aligned}
        P(\mbox{10 heads}) = \sum_{i = 0}^{n-1}P(\mbox{10 heads }| p=\frac{i}{n})(p=\frac{i}{n}) \\=
        \sum_{i = 0}^{n-1}{\frac{i}{n}}^{10}\cdot\frac{1}{n}
    \end{aligned}\]</span></p>
<p>So, if we chose what <span class="math inline">\(n\)</span> was, then we would have all the parts to solve the problem. Looking back at the way we defined S, as <span class="math inline">\(n\)</span> becomes very large, the set S becomes more and more similar to <span class="math inline">\([0,1]\)</span>. Thus, if we take the limit as <span class="math inline">\(n\)</span> approaches <span class="math inline">\(\infty\)</span>, we will get the answer.</p>
<p>Note that our expression for <span class="math inline">\(P(10 \mbox{ heads})\)</span> is a Riemann Sum, so:</p>
<p><span class="math display">\[\begin{aligned}
        P(\mbox{10 heads}) = \lim_{n\to\infty}\sum_{i = 0}^{n-1}{\frac{i}{n}}^{10}\cdot\frac{1}{n} \\=
        \int_0^1x^{10}dx \\=
        \frac{1}{11}
    \end{aligned}\]</span></p>
<p>Now we can substitute in all of the stuff we have computed to get the answer:</p>
<p><span class="math display">\[\begin{aligned}
        P(\mbox{11th heads }|\mbox{ 10 heads}) = \lim_{n\to\infty}\sum_{i = 0}^{n-1}P(\mbox{11th heads }|\mbox{ 10 heads} \cap p=\frac{i}{n})(p=\frac{i}{n}|\mbox{ 10 heads}) \\=
        \lim_{n\to\infty}\sum_{i = 0}^{n-1}\frac{i}{n} \cdot {\frac{i}{n}}^{10} \cdot \frac{\frac{1}{n}}{\frac{1}{11}} \\=
        11 \cdot \lim_{n\to\infty}\sum_{i = 0}^{n-1}{\frac{i}{n}}^{11} \cdot \frac{1}{n} \\=
        11 \cdot \int_0^1x^{11}dx \\=
        \frac{11}{12}
    \end{aligned}\]</span></p>
<p>The question asks for the sum of the numerator and the denominator of the fraction, so the answer is <span class="math inline">\(11 + 12 = \boxed{23}\)</span>.</p>
<p>It makes sense that the answer should be close to 1 because if a coin lands heads 10 times in a row, it is more likely to be weighted heavily towards heads. So, as we observe more and more heads, our estimate of what the true bias of the coin is should go up. You can also think of it as if we took a bunch of coins, flipped them 10 times, and then threw out the ones that did not flip 10 heads. The average bias of the remaining coins would most likely be heavily in favor of heads.</p>
<p>You did not have to do all this defining our own set and doing Riemann Sums business if you saw immediately what the integrals were going to be. However, if you did not know what to do, then imagining the problem dealt with a finite set instead of an infinite set and thinking about how you would solve it can help.</p>
<h1 id="inference-2">Inference #2</h1>
<div class="tcolorbox">
<p>Johnny has a deck of 100 cards, all of which are initially black. An integer n is picked at random between 0 and 100, inclusive, and Johnny paints n of the 100 cards red. Johnny shuffles the cards and starts drawing them from the deck. What is the least number of red cards Johnny has to draw before the probability that all the remaining cards are red is greater than .5?</p>
</div>
<p>If all the cards are red, that means that <span class="math inline">\(n = 100\)</span>, so what we want is the least whole number <span class="math inline">\(k\)</span> that satisfies the inequality <span class="math inline">\(P(n = 100 |\mbox{ first k are red}) &gt; .5\)</span>. To get an expression for <span class="math inline">\(P(n = 100 |\mbox{ first k are red})\)</span>, let’s start with Bayes’ Theorem:</p>
<p><span class="math display">\[\begin{aligned}
        P(n = 100|\mbox{ first k are red}) = P(\mbox{first k are red}|n = 100)\cdot\frac{P(n=100)}{P(\mbox{first k are red})}
    \end{aligned}\]</span></p>
<p><span class="math inline">\(P(\mbox{first k are red }|n = 100) = 1\)</span> because <span class="math inline">\(n = 100\)</span> means that all the cards are red, so the first <span class="math inline">\(k\)</span> will always be red.</p>
<p><span class="math inline">\(P(n = 100) = \frac{1}{101}\)</span> since <span class="math inline">\(n\)</span> is chosen uniformly at random from 101 options.</p>
<p>To calculate <span class="math inline">\(P(\mbox{first k are red})\)</span>, we will use the Total Probability Theorem:</p>
<p><span class="math display">\[\begin{aligned}
        P(\mbox{first k are red }) = \sum_{i=k}^{100}P(n = i)(\mbox{first k are red }|n = i)
    \end{aligned}\]</span></p>
<p><span class="math inline">\(P(n = i) = \frac{1}{101}\)</span> (same explanation as for <span class="math inline">\(P(n = 100)\)</span>.</p>
<p>The tricky part is <span class="math inline">\(P(\mbox{first k are red}|n = i)\)</span>. What that is saying is that you draw <span class="math inline">\(k\)</span> cards out of 100, and they are each one of the <span class="math inline">\(i\)</span> red cards in the deck. There are <span class="math inline">\(100 \choose k\)</span> sets of cards that could be the top <span class="math inline">\(k\)</span> cards. Out of those <span class="math inline">\(100 \choose k\)</span> sets, <span class="math inline">\(i \choose k\)</span> of them are all red cards because there are <span class="math inline">\(i\)</span> red cards to choose from. Thus, <span class="math inline">\(P(\mbox{first k are red}|n = i) = \frac{{i \choose k}}{{100 \choose k}}\)</span></p>
<p>(Quick Tip: here we are considering all the cards to be unique because it makes the calculations easier, but some questions are more easily answered when you consider all objects of the same type to be the same. No matter which way you do it, remember to be consistent. Don’t do one calculation where you consider all the red cards to be the same and then another where you consider them to be different.)</p>
<p>Now let’s substitute that back into our expression for <span class="math inline">\(P(\mbox{first k are red})\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
        P(\mbox{first k are red}) = \sum_{i=k}^{100}P(n = i)\cdot P(\mbox{first k are red }|n = i) \\=
        \sum_{i=k}^{100}\frac{1}{101}\cdot\frac{{i \choose k}}{{100 \choose k}} \\=
        \frac{1}{101}\cdot\frac{1}{{100 \choose k}}\cdot\sum_{i=k}^{100}{i \choose k}
    \end{aligned}\]</span></p>
<p>Now we can use a theorem not mentioned in the article, the Baseball Theorem, which says that <span class="math inline">\(\sum_{i=a}^{b}{i \choose a} = {b+1 \choose a+1}\)</span>, to write:</p>
<p><span class="math display">\[\begin{aligned}
        P(\mbox{first k are red}) = \frac{1}{101}\cdot\frac{1}{{100 \choose k}}\cdot\sum_{i=k}^{100}{i \choose k}\\=
        \frac{1}{101}\cdot\frac{{101 \choose k+1}}{{100 \choose k}} \\=
        \frac{1}{101}\cdot\frac{\frac{101!}{(k+1)!(100-k)!}}{\frac{100!}{k!(100-k)!}} \\=
        \frac{1}{101}\cdot\frac{101!\cdot k!}{100!\cdot(k+1)!} \\=
        \frac{1}{101}\cdot\frac{101}{k+1} \\=
        \frac{1}{k+1}
    \end{aligned}\]</span></p>
<p>(Alternatively, you can notice that <span class="math inline">\(\sum_{i=k}^{100}{i \choose k}\)</span> is the same as choosing <span class="math inline">\(k+1\)</span> of the first 101 natural numbers, doing casework on the largest number you choose. For example, if the largest number you choose is <span class="math inline">\(50\)</span>, then there are <span class="math inline">\(49 \choose k\)</span> options for other <span class="math inline">\(k\)</span> numbers since there are 49 natural numbers less than 50 we are choosing <span class="math inline">\(k+1\)</span> numbers total. The largest number you choose can range from <span class="math inline">\(k+1\)</span> to 101, so <span class="math inline">\({101 \choose k+1} = \sum_{i=k+1}^{101}{i-1 \choose k} = \sum_{i=k}^{100}{i \choose k}\)</span>.)</p>
<p>Finally, we can plug everything into our equation for <span class="math inline">\(P(n = 100 |\mbox{ first k are red})\)</span> to obtain:</p>
<p><span class="math display">\[\begin{aligned}
        P(n = 100 |\mbox{ first k are red}) = P(\mbox{first k are red }|n = 100)\cdot\frac{P(n=100)}{P(\mbox{first k are red})} \\=
        1\cdot\frac{\frac{1}{101}}{\frac{1}{k+1}} \\=
        \frac{k+1}{101}
    \end{aligned}\]</span></p>
<p>The question asks for the least whole number <span class="math inline">\(k\)</span> that satisfies the inequality <span class="math inline">\(P(n = 100|\mbox{first k are red}) = \frac{k+1}{101} &gt; .5\)</span>, so the answer is clearly <span class="math inline">\(\boxed{50}\)</span>.</p>
<p>When I first solved the problem, I was surprised by this result; I thought the answer was going to be much larger. Part of this came from flawed logic that should be avoided when dealing with conditional probability questions. For example, the argument "the answer is 99 because after drawing 99 red cards the remaining card can either be red or black, so there is a 50% chance of drawing a red card" could make sense initially, but it assumes that the last card is red or black with equal probability. In fact, there is a much higher chance of drawing red because if we started with 99 red cards and 1 black card, it would be very unlikely that the first 99 cards were all red (1% chance). There is an equal probability of starting with 99 red cards as starting with 100 red cards, but after observing the 99 red cards in a row we should come to the conclusion that 100 red cards is much more likely.</p>
<h1 id="inference-3">Inference #3</h1>
<div class="tcolorbox">
<p><strong>HMMT Feb 2019 Guts #29</strong>: Yannick picks a number N randomly from the set of positive integers such that the probability that n is selected is <span class="math inline">\(2^{-n}\)</span> for each positive integer n. He then puts N identical slips of paper numbered 1 through N into a hat and gives the hat to Annie. Annie does not know the value of N, but she draws one of the slips uniformly at random and discovers that it is the number 2. What is the expected value of N given Annie’s information? (Note: Uses Calculus)</p>
</div>
<p>We want to find <span class="math inline">\(E(N | \mbox{draws 2})\)</span>. Using the definition of expected value and Bayes’ Theorem, we can write</p>
<p><span class="math display">\[\begin{aligned}
        E(N|\mbox{draws 2}) = \sum_{i=2}^\infty{i \cdot P(N=i |\mbox{ draws 2})} \\=
        \sum_{i=2}^\infty{i \cdot P(\mbox{draws 2 }| N=i) \cdot\frac{P(N=i)}{P(\mbox{draws 2})}}
    \end{aligned}\]</span></p>
<p><span class="math inline">\(P(\mbox{draws 2}|N=i) = \frac{1}{i}\)</span> since there are <span class="math inline">\(i\)</span> slips and Annie chooses one of them at random (technically if <span class="math inline">\(i&lt;2\)</span> than it would be 0 but in our sum <span class="math inline">\(i\)</span> starts at 2).</p>
<p><span class="math inline">\(P(N=i) = 2^{-i}\)</span> because that is given in the problem.</p>
<p>Can you guess what we are going to do to compute <span class="math inline">\(P(\mbox{draws 2})\)</span>? Yes, for the fifth time we are going to use the Total Probability Theorem.</p>
<p><span class="math display">\[\begin{aligned}
        P(\mbox{draws 2}) = \sum_{i=2}^\infty{P(\mbox{draws 2 }|N=i) \cdot P(N=i)} \\=
        \sum_{i=2}^\infty{\frac{1}{i} \cdot2^{-i}}
    \end{aligned}\]</span></p>
<p>This sum looks like a geometric series with a common ratio of <span class="math inline">\(\frac{1}{2}\)</span>, but unfortunately we have that <span class="math inline">\(\frac{1}{i}\)</span> messing it up. In order to get rid of the <span class="math inline">\(\frac{1}{i}\)</span>, we need to do something that will result in each term being multiplied by i. It will be easier to see if we replace <span class="math inline">\(\frac{1}{2}\)</span> with <span class="math inline">\(x\)</span> and then plug in <span class="math inline">\(\frac{1}{2}\)</span> for <span class="math inline">\(x\)</span> later:</p>
<p><span class="math display">\[\begin{aligned}
        f(x) = \sum_{i=2}^\infty{\frac{x^i}{i}}
    \end{aligned}\]</span></p>
<p>and <span class="math inline">\(P(\mbox{draws 2}) = f(\frac{1}{2})\)</span>.</p>
<p>Now it is clear that we have to differentiate:</p>
<p><span class="math display">\[\begin{aligned}
        f&#39;(x) = \sum_{i=2}^\infty{\frac{i\cdot x^{i-1}}{i}} \\= \sum_{i=2}^\infty{x^{i-1}} \\=
        \sum_{i=1}^\infty{x^i} \\=
        \frac{1}{1-x} - 1 &amp;&amp;\text{provided that } |x| &lt; 1 \text{, which it is.}
    \end{aligned}\]</span></p>
<p>To solve for <span class="math inline">\(f(x)\)</span>, integrate:</p>
<p><span class="math display">\[\begin{aligned}
        \int{f&#39;(x)}dx = \int{(\frac{1}{1-x} - 1)}dx
    \end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
        f(x) = -\ln(1-x) - x + C
    \end{aligned}\]</span></p>
<p>To solve for <span class="math inline">\(C\)</span>, plug in 0 for <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
        f(0) = -\ln(1-0) - 0 + C = \frac{1}{1-0} - 1 = 0
    \end{aligned}\]</span></p>
<p>Thus, <span class="math inline">\(C = 0\)</span>, <span class="math inline">\(f(x) = -\ln(1-x) - x\)</span>, and <span class="math inline">\(P(\mbox{draws 2}) = f(\frac{1}{2}) = \ln(2) - \frac{1}{2}\)</span></p>
<p>Now we can go back to our equation to compute <span class="math inline">\(E(N |\mbox{ draws 2})\)</span></p>
<p><span class="math display">\[\begin{aligned}
        E(N |\mbox{ draws 2}) =
        \sum_{i=2}^\infty{i \cdot P(\mbox{draws 2 }| N=i) \cdot\frac{P(N=i)}{P(\mbox{draws 2})}} \\=
        \sum_{i=2}^\infty{i \cdot\frac{1}{i} \cdot \frac{2^{-i}}{\ln(2) - \frac{1}{2}}} \\=
        \frac{1}{\ln(2) - \frac{1}{2}}\cdot\sum_{i=2}^\infty{{\frac{1}{2}} ^ i} \\=
        \frac{1}{\ln(2) - \frac{1}{2}}\cdot\frac{1}{4}\cdot\frac{1}{1-\frac{1}{2}} &amp;&amp; \text{(sum of geometric series)} \\=
        \frac{1}{\ln(2) - \frac{1}{2}}\cdot\frac{1}{4}\cdot2 \\=
        \boxed{\frac{1}{2\ln(2) - 1}}
    \end{aligned}\]</span></p>

</main>
</body>

<script>
var maxHeight = 0;

$(".row").each(function () {
    if ($(this).height() > maxHeight) {
        maxHeight = $(this).height();
    }
});

$(".row").height(maxHeight);
</script>


<footer class="blog-footer">
  <a href="https://us7.list-manage.com/contact-form?u=bd1a0a18ff760b00bf541b12d&form_id=b47981c22ebecdf4a32acfec1a5f0fc7">Contact Us</a> | <a href="https://docs.google.com/forms/d/e/1FAIpQLScARm0MoVGwxdH0w9PWmZNzjIf4CUJPzLcqDJ0UGob88ALIBg/viewform">Join Us</a><br>
Copyright © 2021 Math and CS Research.
</footer>
  </body>
</html>
</body><script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script></html>
